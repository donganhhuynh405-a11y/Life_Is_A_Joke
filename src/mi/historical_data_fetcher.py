"""
Historical Data Fetcher for ML Training

–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é —Ç–æ—Ä–≥—É–µ–º—ã—Ö —Ä—ã–Ω–∫–æ–≤ –æ—Ç –º–æ–º–µ–Ω—Ç–∞ –∏—Ö –ø–æ—è–≤–ª–µ–Ω–∏—è
–¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω–µ –¥–∞–Ω–Ω—ã—Ö.
"""

import json
import logging
import asyncio
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class HistoricalDataFetcher:
    """
    –ó–∞–≥—Ä—É–∑—á–∏–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –±–∏—Ä–∂–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π
    """

    def __init__(self, exchange, cache_dir: str = "/var/lib/trading-bot/historical_data"):
        """
        Args:
            exchange: ExchangeAdapter instance
            cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        self.exchange = exchange
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # –î–∞—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç –Ω–∞ Binance
        self.symbol_launch_dates = {
            'BTCUSDT': '2017-08-17',  # BTC —Ç–æ—Ä–≥—É–µ—Ç—Å—è —Å –Ω–∞—á–∞–ª–∞ Binance
            'ETHUSDT': '2017-08-17',  # ETH —Ç–∞–∫–∂–µ —Å –Ω–∞—á–∞–ª–∞
            'BNBUSDT': '2017-11-06',  # BNB ICO
            'ADAUSDT': '2018-04-17',  # ADA –ª–∏—Å—Ç–∏–Ω–≥
            'SOLUSDT': '2020-08-11',  # SOL –ª–∏—Å—Ç–∏–Ω–≥
            'XRPUSDT': '2018-05-04',  # XRP –ª–∏—Å—Ç–∏–Ω–≥
            'DOTUSDT': '2020-08-19',  # DOT –ª–∏—Å—Ç–∏–Ω–≥
            'DOGEUSDT': '2019-07-05',  # DOGE –ª–∏—Å—Ç–∏–Ω–≥
            'AVAXUSDT': '2020-09-22',  # AVAX –ª–∏—Å—Ç–∏–Ω–≥
            'MATICUSDT': '2019-04-26',  # MATIC –ª–∏—Å—Ç–∏–Ω–≥
        }

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –¥–∞—Ç–∞ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–Ω–∞—á–∞–ª–æ Binance)
        self.default_start_date = '2017-08-17'

    def get_symbol_start_date(self, symbol: str) -> datetime:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–∞—Ç—É –Ω–∞—á–∞–ª–∞ —Ç–æ—Ä–≥–æ–≤–ª–∏ —Å–∏–º–≤–æ–ª–∞

        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'BTCUSDT')

        Returns:
            datetime –æ–±—ä–µ–∫—Ç —Å –¥–∞—Ç–æ–π –Ω–∞—á–∞–ª–∞
        """
        date_str = self.symbol_launch_dates.get(symbol, self.default_start_date)
        return datetime.strptime(date_str, '%Y-%m-%d')

    def _get_cache_path(self, symbol: str, timeframe: str) -> Path:
        """–ü—É—Ç—å –∫ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É"""
        return self.cache_dir / f"{symbol}_{timeframe}.parquet"

    def _get_metadata_path(self, symbol: str, timeframe: str) -> Path:
        """–ü—É—Ç—å –∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –∫—ç—à–∞"""
        return self.cache_dir / f"{symbol}_{timeframe}_meta.json"

    async def fetch_full_history(
        self,
        symbol: str,
        timeframe: str = '1h',
        force_reload: bool = False
    ) -> Optional[pd.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é —Å–∏–º–≤–æ–ª–∞ –æ—Ç –º–æ–º–µ–Ω—Ç–∞ –µ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è

        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º (1m, 5m, 15m, 1h, 4h, 1d)
            force_reload: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à)

        Returns:
            DataFrame —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        cache_path = self._get_cache_path(symbol, timeframe)
        metadata_path = self._get_metadata_path(symbol, timeframe)

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à
        if not force_reload and cache_path.exists():
            try:
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –∫—ç—à–∞
                if metadata_path.exists():
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)

                    last_update = datetime.fromisoformat(metadata['last_update'])
                    # –ï—Å–ª–∏ –∫—ç—à —Å–≤–µ–∂–∏–π (< 24 —á–∞—Å–æ–≤), –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ
                    if datetime.now() - last_update < timedelta(hours=24):
                        logger.info(f"üìÇ Loading {symbol} {timeframe} from cache")
                        df = pd.read_parquet(cache_path)
                        logger.info(
                            f"‚úÖ Loaded {len(df)} candles from cache (from {df.index[0]} to {df.index[-1]})")
                        return df
            except Exception as e:
                logger.warning(f"Failed to load cache for {symbol}: {e}")

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å –±–∏—Ä–∂–∏
        logger.info(f"üîÑ Fetching full history for {symbol} {timeframe}...")

        start_date = self.get_symbol_start_date(symbol)
        end_date = datetime.now()

        # –†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–≤–µ—á–µ–π
        timeframe_minutes = self._timeframe_to_minutes(timeframe)
        total_minutes = int((end_date - start_date).total_seconds() / 60)
        expected_candles = total_minutes // timeframe_minutes

        logger.info(
            f"üìä Expected ~{expected_candles:,} candles from "
            f"{start_date.date()} to {end_date.date()}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞–∫–µ—Ç–∞–º–∏ (–º–∞–∫—Å 1000 —Å–≤–µ—á–µ–π –∑–∞ —Ä–∞–∑)
        all_data = []
        current_start = start_date
        limit = 1000

        while current_start < end_date:
            try:
                # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–∫–µ—Ç
                candles = self.exchange.get_klines(
                    symbol=symbol,
                    interval=timeframe,
                    limit=limit,
                    start_time=int(current_start.timestamp() * 1000)
                )

                if not candles or len(candles) == 0:
                    logger.warning(f"No more data available from {current_start}")
                    break

                all_data.extend(candles)

                # –ü–æ—Å–ª–µ–¥–Ω—è—è timestamp
                last_timestamp = candles[-1][0] / 1000
                current_start = datetime.fromtimestamp(
                    last_timestamp) + timedelta(minutes=timeframe_minutes)

                logger.info(f"üì• Loaded {len(all_data):,} candles so far...")

                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∑–∏—Ç—å API
                await asyncio.sleep(0.1)

                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ —á–µ–º limit, –∑–Ω–∞—á–∏—Ç –¥–æ—Å—Ç–∏–≥–ª–∏ –∫–æ–Ω—Ü–∞
                if len(candles) < limit:
                    break

            except Exception as e:
                logger.error(f"Error fetching data from {current_start}: {e}")
                # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –¥—Ä—É–≥–æ–π —Ç–æ—á–∫–∏
                current_start += timedelta(days=1)
                continue

        if not all_data:
            logger.error(f"Failed to fetch any data for {symbol}")
            return None

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ DataFrame
        df = self._convert_to_dataframe(all_data)

        logger.info(f"‚úÖ Fetched {len(df):,} candles for {symbol} {timeframe}")
        logger.info(f"üìÖ Date range: {df.index[0]} to {df.index[-1]}")

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à
        try:
            df.to_parquet(cache_path)

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'symbol': symbol,
                'timeframe': timeframe,
                'candles': len(df),
                'start_date': df.index[0].isoformat(),
                'end_date': df.index[-1].isoformat(),
                'last_update': datetime.now().isoformat()
            }
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            logger.info(f"üíæ Cached data for {symbol} {timeframe}")
        except Exception as e:
            logger.warning(f"Failed to cache data: {e}")

        return df

    def _timeframe_to_minutes(self, timeframe: str) -> int:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å timeframe –≤ –º–∏–Ω—É—Ç—ã"""
        mapping = {
            '1m': 1,
            '3m': 3,
            '5m': 5,
            '15m': 15,
            '30m': 30,
            '1h': 60,
            '2h': 120,
            '4h': 240,
            '6h': 360,
            '8h': 480,
            '12h': 720,
            '1d': 1440,
            '3d': 4320,
            '1w': 10080,
            '1M': 43200,
        }
        return mapping.get(timeframe, 60)

    def _convert_to_dataframe(self, candles: List) -> pd.DataFrame:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å raw candles –≤ pandas DataFrame

        Args:
            candles: List of [timestamp, open, high, low, close, volume, ...]

        Returns:
            DataFrame with OHLCV data
        """
        df = pd.DataFrame(candles, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades',
            'taker_buy_base', 'taker_buy_quote', 'ignore'
        ])

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å timestamp –≤ datetime
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ float
        for col in ['open', 'high', 'low', 'close', 'volume']:
            df[col] = df[col].astype(float)

        # –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        df = df[['open', 'high', 'low', 'close', 'volume']]

        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
        df = df[~df.index.duplicated(keep='last')]

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        df.sort_index(inplace=True)

        return df

    async def update_cached_data(self, symbol: str,
                                 timeframe: str = '1h') -> Optional[pd.DataFrame]:
        """
        –û–±–Ω–æ–≤–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤—ã–º–∏ —Å–≤–µ—á–∞–º–∏

        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º

        Returns:
            Updated DataFrame
        """
        cache_path = self._get_cache_path(symbol, timeframe)

        if not cache_path.exists():
            # –ù–µ—Ç –∫—ç—à–∞, –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é
            return await self.fetch_full_history(symbol, timeframe)

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫—ç—à
        try:
            df = pd.read_parquet(cache_path)
            last_timestamp = df.index[-1]

            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            logger.info(f"üîÑ Updating {symbol} {timeframe} from {last_timestamp}")

            candles = self.exchange.get_klines(
                symbol=symbol,
                interval=timeframe,
                limit=1000,
                start_time=int(last_timestamp.timestamp() * 1000)
            )

            if candles and len(candles) > 0:
                new_df = self._convert_to_dataframe(candles)

                # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å
                df = pd.concat([df, new_df])
                df = df[~df.index.duplicated(keep='last')]
                df.sort_index(inplace=True)

                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
                df.to_parquet(cache_path)

                logger.info(f"‚úÖ Updated {symbol}: added {len(new_df)} new candles, total {len(df)}")

            return df

        except Exception as e:
            logger.error(f"Failed to update cached data: {e}")
            return None

    def get_cache_info(self, symbol: str, timeframe: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        metadata_path = self._get_metadata_path(symbol, timeframe)

        if not metadata_path.exists():
            return None

        try:
            with open(metadata_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to read metadata: {e}")
            return None
