"""
Market-Specific ML Model Trainer

–û–±—É—á–∞–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é ML –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ—Ä–≥—É–µ–º–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
–Ω–∞ –ø–æ–ª–Ω–æ–π –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω–µ –¥–∞–Ω–Ω—ã—Ö —ç—Ç–æ–≥–æ —Ä—ã–Ω–∫–∞.
"""

import os
import json
import pickle
import logging
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)


@dataclass
class ModelMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
    symbol: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    train_samples: int
    test_samples: int
    training_date: str
    model_version: str = "1.0"
    
    def to_dict(self) -> Dict:
        return asdict(self)


class MarketSpecificTrainer:
    """
    –¢—Ä–µ–Ω–µ—Ä ML –º–æ–¥–µ–ª–µ–π —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—ã–Ω–∫–∞
    """
    
    def __init__(self, models_dir: str = "/var/lib/trading-bot/models"):
        """
        Args:
            models_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.min_training_samples = 1000
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏—á–µ–π
        self.lookback_period = 60  # –°–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 —Å–≤–µ—á–µ–π
        self.feature_columns = [
            'open', 'high', 'low', 'close', 'volume',
            'rsi', 'macd', 'macd_signal', 'bb_upper', 'bb_middle', 'bb_lower',
            'atr', 'volume_sma'
        ]
    
    def _get_model_path(self, symbol: str) -> Path:
        """–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏"""
        symbol_dir = self.models_dir / symbol
        symbol_dir.mkdir(exist_ok=True)
        return symbol_dir / "model.pkl"
    
    def _get_metrics_path(self, symbol: str) -> Path:
        """–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–µ—Ç—Ä–∏–∫"""
        symbol_dir = self.models_dir / symbol
        symbol_dir.mkdir(exist_ok=True)
        return symbol_dir / "metrics.json"
    
    def _get_scaler_path(self, symbol: str) -> Path:
        """–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É scaler"""
        symbol_dir = self.models_dir / symbol
        symbol_dir.mkdir(exist_ok=True)
        return symbol_dir / "scaler.pkl"
    
    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —Ñ–∏—á–µ–π
        
        Args:
            df: DataFrame —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        """
        df = df.copy()
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        df['macd'] = exp1 - exp2
        df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
        
        # Bollinger Bands
        sma_20 = df['close'].rolling(window=20).mean()
        std_20 = df['close'].rolling(window=20).std()
        df['bb_upper'] = sma_20 + (std_20 * 2)
        df['bb_middle'] = sma_20
        df['bb_lower'] = sma_20 - (std_20 * 2)
        
        # ATR
        high_low = df['high'] - df['low']
        high_close = (df['high'] - df['close'].shift()).abs()
        low_close = (df['low'] - df['close'].shift()).abs()
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        df['atr'] = true_range.rolling(window=14).mean()
        
        # Volume SMA
        df['volume_sma'] = df['volume'].rolling(window=20).mean()
        
        # –ó–∞–ø–æ–ª–Ω–∏—Ç—å NaN
        df.fillna(method='bfill', inplace=True)
        df.fillna(method='ffill', inplace=True)
        
        return df
    
    def prepare_features_and_labels(
        self, 
        df: pd.DataFrame, 
        prediction_horizon: int = 1
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ñ–∏—á–∏ –∏ –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
            prediction_horizon: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–µ—á–µ–π –≤–ø–µ—Ä–µ–¥)
            
        Returns:
            (X, y) - —Ñ–∏—á–∏ –∏ –º–µ—Ç–∫–∏
        """
        # –†–∞—Å—á–µ—Ç –±—É–¥—É—â–µ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        df['future_return'] = df['close'].shift(-prediction_horizon) / df['close'] - 1
        
        # –ú–µ—Ç–∫–∏: 1 –µ—Å–ª–∏ —Ü–µ–Ω–∞ –≤—ã—Ä–∞—Å—Ç–µ—Ç >0.5%, 0 –µ—Å–ª–∏ —É–ø–∞–¥–µ—Ç >0.5%, –∏–Ω–∞—á–µ –Ω–µ—Ç —Å–∏–≥–Ω–∞–ª–∞
        df['label'] = 0
        df.loc[df['future_return'] > 0.005, 'label'] = 1  # UP
        df.loc[df['future_return'] < -0.005, 'label'] = -1  # DOWN
        
        # –£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å NaN
        df_clean = df.dropna()
        
        if len(df_clean) < self.min_training_samples:
            raise ValueError(f"Insufficient data: {len(df_clean)} < {self.min_training_samples}")
        
        # –°–æ–∑–¥–∞—Ç—å sequences (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ)
        X_list = []
        y_list = []
        
        for i in range(self.lookback_period, len(df_clean)):
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ lookback_period —Å–≤–µ—á–µ–π
            window = df_clean.iloc[i-self.lookback_period:i]
            
            # –ò–∑–≤–ª–µ—á—å —Ñ–∏—á–∏
            features = window[self.feature_columns].values.flatten()
            X_list.append(features)
            
            # –ú–µ—Ç–∫–∞
            label = df_clean.iloc[i]['label']
            y_list.append(label)
        
        X = np.array(X_list)
        y = np.array(y_list)
        
        logger.info(f"‚úÖ Prepared {len(X)} samples with {X.shape[1]} features each")
        
        return X, y
    
    def train_model(
        self, 
        symbol: str, 
        df: pd.DataFrame,
        test_size: float = 0.2
    ) -> Optional[ModelMetrics]:
        """
        –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª
            df: DataFrame —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            test_size: –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (0.0 - 1.0)
            
        Returns:
            ModelMetrics —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"üéì Training model for {symbol} on {len(df)} candles")
        
        try:
            # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df = self.calculate_technical_indicators(df)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
            X, y = self.prepare_features_and_labels(df)
            
            # –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ train/test
            split_idx = int(len(X) * (1 - test_size))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            logger.info(f"üìä Train: {len(X_train)}, Test: {len(X_test)}")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å (Random Forest –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
            from sklearn.ensemble import RandomForestClassifier
            
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=10,
                random_state=42,
                n_jobs=-1
            )
            
            logger.info("üîÑ Training Random Forest...")
            model.fit(X_train_scaled, y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            y_pred = model.predict(X_test_scaled)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            
            # –î–ª—è multi-class –Ω—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å average
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
            
            metrics = ModelMetrics(
                symbol=symbol,
                accuracy=accuracy,
                precision=precision,
                recall=recall,
                f1_score=f1,
                train_samples=len(X_train),
                test_samples=len(X_test),
                training_date=datetime.now().isoformat()
            )
            
            logger.info(f"‚úÖ Model trained successfully!")
            logger.info(f"   Accuracy: {accuracy:.4f}")
            logger.info(f"   Precision: {precision:.4f}")
            logger.info(f"   Recall: {recall:.4f}")
            logger.info(f"   F1 Score: {f1:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å
            model_path = self._get_model_path(symbol)
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å scaler
            scaler_path = self._get_scaler_path(symbol)
            with open(scaler_path, 'wb') as f:
                pickle.dump(scaler, f)
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
            metrics_path = self._get_metrics_path(symbol)
            with open(metrics_path, 'w') as f:
                json.dump(metrics.to_dict(), f, indent=2)
            
            logger.info(f"üíæ Model saved to {model_path}")
            
            return metrics
            
        except Exception as e:
            logger.error(f"‚ùå Failed to train model for {symbol}: {e}", exc_info=True)
            return None
    
    def load_model(self, symbol: str) -> Optional[Tuple]:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª
            
        Returns:
            (model, scaler, metrics) –∏–ª–∏ None
        """
        model_path = self._get_model_path(symbol)
        scaler_path = self._get_scaler_path(symbol)
        metrics_path = self._get_metrics_path(symbol)
        
        if not model_path.exists():
            logger.warning(f"No trained model found for {symbol}")
            return None
        
        try:
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å scaler
            scaler = None
            if scaler_path.exists():
                with open(scaler_path, 'rb') as f:
                    scaler = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
            metrics = None
            if metrics_path.exists():
                with open(metrics_path, 'r') as f:
                    metrics = json.load(f)
            
            logger.info(f"‚úÖ Loaded model for {symbol} (accuracy: {metrics.get('accuracy', 'N/A') if metrics else 'N/A'})")
            
            return model, scaler, metrics
            
        except Exception as e:
            logger.error(f"Failed to load model for {symbol}: {e}")
            return None
    
    def predict(
        self, 
        symbol: str, 
        recent_data: pd.DataFrame
    ) -> Optional[Dict]:
        """
        –°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å–∏–º–≤–æ–ª–∞
        
        Args:
            symbol: –¢–æ—Ä–≥–æ–≤—ã–π —Å–∏–º–≤–æ–ª
            recent_data: DataFrame —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–º–∏–Ω–∏–º—É–º lookback_period —Å–≤–µ—á–µ–π)
            
        Returns:
            Dict —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        """
        model_data = self.load_model(symbol)
        if model_data is None:
            return None
        
        model, scaler, metrics = model_data
        
        try:
            # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df = self.calculate_technical_indicators(recent_data)
            
            # –í–∑—è—Ç—å –ø–æ—Å–ª–µ–¥–Ω–µ–µ –æ–∫–Ω–æ
            if len(df) < self.lookback_period:
                logger.warning(f"Insufficient data for prediction: {len(df)} < {self.lookback_period}")
                return None
            
            window = df.iloc[-self.lookback_period:]
            features = window[self.feature_columns].values.flatten().reshape(1, -1)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            if scaler:
                features = scaler.transform(features)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = model.predict(features)[0]
            probabilities = model.predict_proba(features)[0]
            
            # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
            classes = model.classes_
            class_idx = np.where(classes == prediction)[0][0]
            confidence = probabilities[class_idx]
            
            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
            signal_map = {
                1: 'BUY',
                -1: 'SELL',
                0: 'HOLD'
            }
            
            result = {
                'signal': signal_map.get(prediction, 'HOLD'),
                'confidence': float(confidence),
                'prediction': int(prediction),
                'probabilities': {
                    signal_map.get(cls, f'class_{cls}'): float(prob)
                    for cls, prob in zip(classes, probabilities)
                },
                'model_accuracy': metrics.get('accuracy', 0.0) if metrics else 0.0,
                'timestamp': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Prediction failed for {symbol}: {e}", exc_info=True)
            return None
    
    def get_model_info(self, symbol: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ —Å–∏–º–≤–æ–ª–∞"""
        metrics_path = self._get_metrics_path(symbol)
        
        if not metrics_path.exists():
            return None
        
        try:
            with open(metrics_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to read model info: {e}")
            return None
